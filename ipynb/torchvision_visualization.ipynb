{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6699c7b9",
   "metadata": {},
   "source": [
    "# TorchVision Dataset Visualization\n",
    "Demonstrate ACT's TorchVision loader with MNIST/CIFAR10 perturbation visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10301b17",
   "metadata": {},
   "source": [
    "## TorchVision MNIST Perturbation Visualization\n",
    "\n",
    "This section demonstrates:\n",
    "- Loading MNIST dataset with ACT's TorchVision loader\n",
    "- Creating input specifications with epsilon perturbations\n",
    "- Visualizing perturbed images with model predictions\n",
    "- Color-coded prediction feedback (green=correct, red=incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d25c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACT MNIST: Visualize Specs and Test Robustness\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup ACT paths using path_config\n",
    "act_root = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "if act_root not in sys.path:\n",
    "    sys.path.insert(0, act_root)\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from act.front_end.torchvision_loader.data_model_loader import load_dataset_model_pair\n",
    "from act.front_end.torchvision_loader.create_specs import TorchVisionSpecCreator\n",
    "from act.front_end.specs import InKind\n",
    "\n",
    "# Load MNIST + simple_cnn\n",
    "print(\"Loading MNIST + simple_cnn...\\n\")\n",
    "pair = load_dataset_model_pair(\"MNIST\", \"simple_cnn\", split=\"test\", batch_size=1, shuffle=False)\n",
    "model, dataset = pair['model'], pair['dataset']\n",
    "model.eval()\n",
    "\n",
    "# Create spec creator with epsilon=0.05 for visible perturbations\n",
    "spec_creator = TorchVisionSpecCreator(config_dict={\n",
    "    'epsilons': [0.05],\n",
    "    'input_kinds': ['BOX'],\n",
    "    'output_kinds': ['MARGIN_ROBUST']\n",
    "})\n",
    "\n",
    "# Generate specs for 2 samples\n",
    "results = spec_creator.create_specs_for_data_model_pairs(\n",
    "    dataset_names=[\"MNIST\"], model_names=[\"simple_cnn\"], \n",
    "    num_samples=2, start_index=0, split=\"test\", validate_shapes=False\n",
    ")\n",
    "\n",
    "_, _, _, input_tensors, spec_pairs = results[0]\n",
    "\n",
    "# Visualize: Original + 3 Perturbed versions per image\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('ACT Robustness Testing: Original vs Perturbed Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "for sample_idx in range(2):\n",
    "    img_tensor = input_tensors[sample_idx]\n",
    "    _, label = dataset[sample_idx]\n",
    "    \n",
    "    # Original image - handle both grayscale and RGB\n",
    "    img_np = img_tensor.numpy()\n",
    "    if img_np.ndim == 3:  # (C, H, W) format\n",
    "        if img_np.shape[0] == 1:  # Grayscale\n",
    "            img_display = img_np.squeeze()\n",
    "            cmap = 'gray'\n",
    "        else:  # RGB - transpose to (H, W, C)\n",
    "            img_display = np.transpose(img_np, (1, 2, 0))\n",
    "            cmap = None\n",
    "    else:  # Already (H, W)\n",
    "        img_display = img_np\n",
    "        cmap = 'gray'\n",
    "    \n",
    "    axes[sample_idx, 0].imshow(img_display, cmap=cmap, vmin=0, vmax=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor.unsqueeze(0)).argmax(dim=1).item()\n",
    "    color = 'green' if pred == label else 'red'\n",
    "    axes[sample_idx, 0].set_title(f'Original\\nGT:{label} Pred:{pred}', fontweight='bold', color=color)\n",
    "    axes[sample_idx, 0].axis('off')\n",
    "    \n",
    "    # Get spec for this sample\n",
    "    input_spec = spec_pairs[sample_idx][0]\n",
    "    lb, ub = input_spec.lb.numpy().squeeze(), input_spec.ub.numpy().squeeze()\n",
    "    \n",
    "    # Detect image shape from original tensor\n",
    "    if img_np.ndim == 3:\n",
    "        img_shape = img_np.shape  # (C, H, W)\n",
    "    else:\n",
    "        img_shape = (1, *img_np.shape)  # (1, H, W)\n",
    "    \n",
    "    # Generate 3 perturbed samples: lower bound, center, upper bound\n",
    "    perturbed_samples = [\n",
    "        lb,  # Lower bound\n",
    "        (lb + ub) / 2,  # Mid-point\n",
    "        ub   # Upper bound\n",
    "    ]\n",
    "    \n",
    "    for pert_idx, pert_img in enumerate(perturbed_samples):\n",
    "        # Run inference on perturbed image\n",
    "        pert_tensor = torch.from_numpy(pert_img).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pert_pred = model(pert_tensor).argmax(dim=1).item()\n",
    "        \n",
    "        # Visualize - reshape to original image shape\n",
    "        pert_reshaped = pert_img.reshape(img_shape)\n",
    "        if pert_reshaped.ndim == 3:\n",
    "            if pert_reshaped.shape[0] == 1:  # Grayscale\n",
    "                pert_display = pert_reshaped.squeeze()\n",
    "                pert_cmap = 'gray'\n",
    "            else:  # RGB\n",
    "                pert_display = np.transpose(pert_reshaped, (1, 2, 0))\n",
    "                pert_cmap = None\n",
    "        else:\n",
    "            pert_display = pert_reshaped\n",
    "            pert_cmap = 'gray'\n",
    "        \n",
    "        axes[sample_idx, pert_idx + 1].imshow(pert_display, cmap=pert_cmap, vmin=0, vmax=1)\n",
    "        pert_color = 'green' if pert_pred == label else 'red'\n",
    "        pert_names = ['Lower', 'Mid', 'Upper']\n",
    "        axes[sample_idx, pert_idx + 1].set_title(\n",
    "            f'{pert_names[pert_idx]} Bound\\nPred:{pert_pred}',\n",
    "            color=pert_color, fontsize=10\n",
    "        )\n",
    "        axes[sample_idx, pert_idx + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "eps = spec_creator.config['epsilons'][0]\n",
    "spec_kind = spec_pairs[0][0].kind\n",
    "spec_kind_name = spec_kind.name if hasattr(spec_kind, 'name') else str(spec_kind)\n",
    "print(f\"\\n✓ Tested {len(input_tensors)} samples with ε={eps}\")\n",
    "print(f\"  Each sample: 1 original + 3 perturbed versions\")\n",
    "print(f\"  Green = Correct prediction | Red = Incorrect prediction\")\n",
    "print(f\"  Spec type: {spec_kind_name} (Box constraints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a913b",
   "metadata": {},
   "source": [
    "## Creating Custom Verification Bounds\n",
    "\n",
    "Tutorial on creating L∞ perturbation bounds for verification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Common approaches to create verification bounds for images:\n",
    "\n",
    "1. L_infinity (L∞) perturbation: Each pixel can vary by ±ε\n",
    "2. L2 perturbation: Total L2 distance from center ≤ ε\n",
    "3. Box constraints: Manual per-pixel bounds\n",
    "4. Normalized bounds: Apply after normalization (mean/std)\n",
    "\"\"\"\n",
    "\n",
    "def create_linf_bounds(image, epsilon, pixel_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Create L_infinity perturbation bounds around an image.\n",
    "    \n",
    "    Args:\n",
    "        image: torch.Tensor or numpy array (H, W) or (C, H, W)\n",
    "        epsilon: Maximum perturbation per pixel\n",
    "        pixel_range: Valid pixel value range (min, max)\n",
    "    \n",
    "    Returns:\n",
    "        lower_bounds, upper_bounds (numpy arrays, flattened)\n",
    "    \"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy()\n",
    "    \n",
    "    center = image.flatten()\n",
    "    lower = np.clip(center - epsilon, pixel_range[0], pixel_range[1])\n",
    "    upper = np.clip(center + epsilon, pixel_range[0], pixel_range[1])\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "# Example: MNIST with L_infinity perturbation\n",
    "print(\"=\"*70)\n",
    "print(\"Example: MNIST L∞ Perturbation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "mnist_img, mnist_label = dataset[5]\n",
    "epsilon_mnist = 0.05\n",
    "\n",
    "lower_mnist, upper_mnist = create_linf_bounds(mnist_img.squeeze(), epsilon_mnist)\n",
    "\n",
    "print(f\"✓ Dataset returns: (image, label) tuple\")\n",
    "print(f\"✓ Image: MNIST test sample 5\")\n",
    "print(f\"✓ Ground Truth Label: {mnist_label} (explicit integer)\")\n",
    "print(f\"✓ Epsilon (L∞): {epsilon_mnist}\")\n",
    "print(f\"  Original image range: [{mnist_img.min():.3f}, {mnist_img.max():.3f}]\")\n",
    "print(f\"  Lower bounds range: [{lower_mnist.min():.3f}, {lower_mnist.max():.3f}]\")\n",
    "print(f\"  Upper bounds range: [{upper_mnist.min():.3f}, {upper_mnist.max():.3f}]\")\n",
    "print(f\"  Input dimension: {len(lower_mnist)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(mnist_img.squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "axes[0].set_title(f'Original Image\\n★ Ground Truth Label: {mnist_label} ★', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(lower_mnist.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "axes[1].set_title(f'Lower Bound\\n(image - {epsilon_mnist})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(upper_mnist.reshape(28, 28), cmap='gray', vmin=0, vmax=1)\n",
    "axes[2].set_title(f'Upper Bound\\n(image + {epsilon_mnist})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary: TorchVision Approach\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "✓ **LABELS ARE EXPLICIT**: Each dataset item is a (image, label) tuple\n",
    "  - mnist_img, mnist_label = dataset[0]\n",
    "  - The label is an integer (0-9 for MNIST/CIFAR10)\n",
    "  - No ambiguity, no parsing needed!\n",
    "\n",
    "✓ **BOUNDS ARE USER-DEFINED**: You create them based on verification goals\n",
    "  - L∞ ball with ε = 0.01 to 0.1 (common: 0.03 for MNIST, 2/255 for CIFAR10)\n",
    "  - Apply before or after normalization (depends on network)\n",
    "  - Always clip to valid pixel range [0, 1]\n",
    "\n",
    "✓ **VERIFICATION PROPERTY**: Combine bounds + label for verification\n",
    "  - Input spec: All images in epsilon-ball around original image\n",
    "  - Output property: Model should predict the ground truth label\n",
    "  - Example: \"For all images within ε=0.03, model predicts digit 5\"\n",
    "\n",
    "This approach is ideal for:\n",
    "  • Custom datasets and models\n",
    "  • Rapid prototyping and exploration\n",
    "  • Educational demonstrations\n",
    "  • Flexible verification requirements\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
